import torch
import datetime
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from SEM_Data import SEM_DATA
from utils import data_split
import argparse
import os
from torch.optim import Adam, SGD, RMSprop, AdamW
import time
from model.u2net import u2net_full_config, u2net_lite_config
from model.unet import UNet
from tqdm import tqdm
from tabulate import tabulate
from utils.train_and_eval import *
from utils.model_initial import *
from loss_fn import *
from torch.amp import GradScaler, autocast
from metrics import Evaluate_Metric
from torch.utils.tensorboard import SummaryWriter
import utils.transforms as T
from torch.optim.lr_scheduler import StepLR


class SODPresetTrain:
    def __init__(self, base_size: Union[int, List[int]], crop_size: int,
                 hflip_prob=0.5, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):
        self.transforms = T.Compose([
            T.ToTensor(),
            T.Resize(base_size),
            T.RandomCrop(crop_size),
            T.RandomHorizontalFlip(hflip_prob),
            T.Normalize(mean=mean, std=std)
        ])

    def __call__(self, img, target):
        data = self.transforms(img, target)
        return data

class SODPresetEval:
    def __init__(self, base_size: Union[int, List[int]], mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):
        self.transforms = T.Compose([
            T.ToTensor(),
            T.Resize(base_size),
            T.Normalize(mean=mean, std=std),
        ])

    def __call__(self, img, target):
        data = self.transforms(img, target)
        return data
    

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

now = time.localtime()
detailed_time_str = time.strftime("%Y-%m-%d_%H:%M:%S", now)
log_name = f"log_{detailed_time_str}"

def main(args):
    initial_time = time.time()

    """å‚æ•°åˆ—è¡¨"""
    params = vars(args)
    params_dict = {}
    params_dict['Parameter'] = [str(p[0]) for p in list(params.items())]
    params_dict['Value'] = [str(p[1]) for p in list(params.items())]
    params_header = ['Parameter', 'Value']
    """æ‰“å°å‚æ•°"""
    print(tabulate(params_dict, headers=params_header, tablefmt="grid"))
    

    # å®šä¹‰è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    batch_size = args.batch_size

    # ç”¨æ¥ä¿å­˜è®­ç»ƒä»¥åŠéªŒè¯è¿‡ç¨‹ä¸­ä¿¡æ¯
    save_scores_path = args.save_scores_path
    if not os.path.exists(save_scores_path):
        os.makedirs(save_scores_path)
        
    """â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”æ¨¡åž‹ é…ç½®â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”"""   
    # åŠ è½½æ¨¡åž‹
    if args.model =="u2net_full":
        model = u2net_full_config()
    elif args.model =="u2net_lite":
        model = u2net_lite_config()
    elif args.model == "unet":
        model = UNet(in_channels=3, n_classes=4, base_channels=64, bilinear=True)
    else:
        raise ValueError("model must be 'u2net_full' or 'u2net_lite' or 'unet'.")

    # # åŠ è½½æƒé‡
    # weights_path = None
    # if weights_path is not None:
    #     model.load_state_dict(torch.load(weights_path))['model']
        
    # åˆå§‹åŒ–æ¨¡åž‹
    kaiming_initial(model)
    model.to(device)

    # ä¼˜åŒ–å™¨
    if args.optimizer == 'AdamW':
        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd) # ä¼šå‡ºçŽ°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±

    elif args.optimizer == 'SGD':
        optimizer = SGD(model.parameters(), lr=args.lr, momentum=0.9)

    elif args.optimizer == 'RMSprop':

        optimizer = RMSprop(model.parameters(), lr=args.lr, alpha=0.9, eps=1e-8)
    else:
        raise ValueError("optimizer must be 'AdamW', 'SGD' or 'RMSprop'.")
    
    # è°ƒåº¦å™¨
    
    
    # æŸå¤±å‡½æ•°
    if args.loss_fn == 'CrossEntropyLoss':
        loss_fn = CrossEntropyLoss()
    elif args.loss_fn == 'DiceLoss':
        loss_fn = DiceLoss()
    elif args.loss_fn == 'FocalLoss':
        loss_fn = Focal_Loss()
    else:
        raise ValueError("loss function must be 'CrossEntropyLoss', 'FocalLoss', 'DiceLoss'.")
    
    # ç¼©æ”¾å™¨
    scaler = torch.cuda.amp.GradScaler() if args.amp else None
    Metrics = Evaluate_Metric()
    
    # æ—¥å¿—ä¿å­˜è·¯å¾„
    save_logs_path = f"./results/logs/{args.model}"
    
    if not os.path.exists(save_logs_path):
        os.makedirs(save_logs_path)
    writer = SummaryWriter(f'{save_logs_path}/{datetime.datetime.now().strftime("%Y-%m-%d_%H:%M:%S")}')

    """â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”åŠ è½½æ•°æ®é›†â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”"""
    train_ratio = args.train_ratio
    val_ratio = args.val_ratio
    
    
    # é¢„å¤„ç†
    # img_compose  =  transforms.Compose([
    #                 transforms.ToTensor(),
    #                 transforms.Resize((320, 320)),
    #                 transforms.GaussianBlur((3, 3)),
    #                 transforms.RandomHorizontalFlip(p=0.5),
    #                 transforms.Normalize(mean=[0.485], std=[0.229])])
    
    # mask_compose =  transforms.Compose([
    #                 transforms.Resize((320, 320)),
    #                 transforms.RandomHorizontalFlip(p=0.5)])
    

    # åˆ’åˆ†æ•°æ®é›†
    if args.small_data is not None:
        train_datasets, val_datasets, test_datasets = data_split.small_data_split_to_train_val_test(args.data_path, 
                                                                                                    num_small_data=args.small_data, 
                                                                                                    # train_ratio=0.8, 
                                                                                                    # val_ratio=0.1, 
                            save_path='/mnt/c/VScode/WS-Hub/WS-U2net/U-2-Net/SEM_DATA/CSV',
                            flag=args.split_flag) 
    
    else:
        train_datasets, val_datasets, test_datasets = data_split.data_split_to_train_val_test(args.data_path, train_ratio=train_ratio, val_ratio=val_ratio,
                            save_path='/mnt/c/VScode/WS-Hub/WS-U2net/U-2-Net/SEM_DATA/CSV',   # ä¿å­˜åˆ’åˆ†å¥½çš„æ•°æ®é›†è·¯å¾„
                            flag=args.split_flag)

    # è¯»å–æ•°æ®é›†
    train_datasets = SEM_DATA(train_datasets, 
                            transforms=SODPresetTrain((320, 320), crop_size=320))
    
    val_datasets = SEM_DATA(val_datasets, 
                            transforms=SODPresetEval((320, 320)))
    
    num_workers = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])
    train_dataloader = DataLoader(train_datasets, 
                                batch_size=batch_size, 
                                shuffle=True, 
                                num_workers=num_workers,
                                pin_memory=True)
    
    val_dataloader = DataLoader(val_datasets, 
                                batch_size=4, 
                                shuffle=False, 
                                num_workers=num_workers,
                                pin_memory=True)
    
    """â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”è®­ç»ƒ éªŒè¯â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”"""
    start_epoch = args.start_epoch
    end_epoch = args.end_epoch


  
    current_mae, current_f1 = float('inf'), 0.0
    
    for epoch in range(start_epoch, end_epoch):
        print(f"âœˆâœˆâœˆâœˆâœˆ epoch : {epoch + 1} / {end_epoch} âœˆâœˆâœˆâœˆâœˆâœˆ")
        print(f"--Training-- ðŸ˜€")
        # è®°å½•æ—¶é—´
        start_time = time.time()
        # è®­ç»ƒ
        total_loss = train_one_epoch(model, 
                                    optimizer, 
                                    epoch, 
                                    train_dataloader, 
                                    device=device, 
                                    loss_fn=loss_fn, 
                                    scaler=scaler) # loss

        
        save_file = {"model": model.state_dict(),
                     "optimizer": optimizer.state_dict(),
                     "Metrics": Metrics.state_dict(),
                     "epoch": epoch,
                     "args": args}
        # æ±‚å¹³å‡
        # train_OM_loss = OM_loss / len(train_dataloader)
        # train_OP_loss = OP_loss / len(train_dataloader)
        # train_IOP_loss = IOP_loss / len(train_dataloader)
        train_mean_loss = total_loss / len(train_dataloader)

        # è®°å½•æ—¥å¿—
        writer.add_scalars('train/Loss', 
                           {'Mean':train_mean_loss, 
                            #  'OM': train_OM_loss, 
                            #  'OP': train_OP_loss, 
                            #  'IOP': train_IOP_loss
                            },
                           epoch)

        # ç»“æŸæ—¶é—´
        end_time = time.time()
        train_cost_time = end_time - start_time

        # æ‰“å°

        print(
            #   f"train_OM_loss: {train_OM_loss:.3f}\n"
            #   f"train_OP_loss: {train_OP_loss:.3f}\n"
            #   f"train_IOP_loss: {train_IOP_loss:.3f}\n"
              f"train_mean_loss: {train_mean_loss:.3f}\n"
              f"train_cost_time: {train_cost_time:.2f}s\n")
        
        # éªŒè¯
        if epoch % args.eval_interval == 0 or epoch == end_epoch - 1:

            print(f"--Validation-- ðŸ˜€")
            # è®°å½•éªŒè¯å¼€å§‹æ—¶é—´
            start_time = time.time()
            # æ¯é—´éš”eval_intervalä¸ªepochéªŒè¯ä¸€æ¬¡ï¼Œå‡å°‘éªŒè¯é¢‘çŽ‡èŠ‚çœè®­ç»ƒæ—¶é—´
            val_mean_loss, Metric_list = evaluate(model, device, val_dataloader, loss_fn, Metrics) # val_loss, recall, precision, f1_scores

            # æ±‚å¹³å‡
            # val_OM_loss = val_OM_loss / len(val_dataloader)
            # val_OP_loss = val_OP_loss / len(val_dataloader)
            # val_IOP_loss = val_IOP_loss / len(val_dataloader)
            val_mean_loss = val_mean_loss / len(val_dataloader)

            # è¯„ä»·æŒ‡æ ‡ metrics = [recall, precision, dice, f1_score]
            val_metrics ={}
            val_metrics[epoch] = epoch
            val_metrics["Recall"] = Metric_list[0]
            val_metrics["Precision"] = Metric_list[1]
            val_metrics["Dice"] = Metric_list[2]
            val_metrics["F1_scores"] = Metric_list[3]

            # éªŒè¯====ç»“æŸæ—¶é—´
            end_time = time.time()
            val_cost_time = end_time - start_time

            # æ‰“å°ç»“æžœ
            print(
                #   f"val_OM_loss: {val_OM_loss:.3f}\n"
                #   f"val_OP_loss: {val_OP_loss:.3f}\n"
                #   f"val_IOP_loss: {val_IOP_loss:.3f}\n"
                  f"val_mean_loss: {val_mean_loss:.3f}\n"
                  f"val_cost_time: {val_cost_time:.2f}s\n\n")
            
            # è®°å½•æ—¥å¿—
            tb = args.tb
            if tb:
                writer.add_scalars('val/Loss', 
                                {'Mean':val_mean_loss},
                                epoch)
                
                writer.add_scalars('val/Dice',
                                {'Mean':val_metrics['Dice'][3],
                                },
                                epoch)
                
                writer.add_scalars('val/Precision', 
                                {'Mean':val_metrics['Precision'][3]},
                                epoch)
                
                writer.add_scalars('val/Recall', 
                                {'Mean':val_metrics['Recall'][3]},
                                epoch)
                writer.add_scalars('val/F1', 
                                {'Mean':val_metrics['F1_scores'][3]},
                                epoch) 
                # writer.add_scalars('val/F2', 
                #                 {'Mean':val_metrics['F2_scores'][0], 
                #                     'OM': val_metrics['F2_scores'][1], 
                #                     'OP': val_metrics['F2_scores'][2], 
                #                     'IOP': val_metrics['F2_scores'][3]},
                #                 epoch)  

                # writer.add_scalars('val/Jaccard_index',
                #                 {'Mean':val_metrics['Jaccard_scores'][0],
                #                     'OM': val_metrics['Jaccard_scores'][1],
                #                     'OP': val_metrics['Jaccard_scores'][2],
                #                     'IOP': val_metrics['Jaccard_scores'][3]},
                #                 epoch)   

                # writer.add_scalars('val/Accuracy',
                #                 {'Mean':val_metrics['Accuracy_scores'][0],
                #                     'OM': val_metrics['Accuracy_scores'][1],
                #                     'OP': val_metrics['Accuracy_scores'][2],
                #                     'IOP': val_metrics['Accuracy_scores'][3]},
                #                 epoch)
            
            
            # ä¿å­˜æŒ‡æ ‡
            metrics_table_header = ['Metrics_Name', 'Mean']
            metrics_table_left = ['Dice', 'Recall', 'Precision', 'F1_scores']
            epoch_s = f"âœˆâœˆâœˆâœˆâœˆ epoch : {epoch + 1} / {end_epoch} âœˆâœˆâœˆâœˆâœˆâœˆ\n"
            model_s = f"model : {args.model} \n"
            lr_s = f"lr : {args.lr} \n"
            wd_s = f"wd : {args.wd} \n"
            loss_fn_s = f"loss_fn : {args.loss_fn} \n"
            time_s = f"time : {datetime.datetime.now().strftime('%Y.%m.%d-%H:%M:%S')} \n"
            cost_s = f"cost_time :{val_cost_time / 60:.2f}mins \n"
            
            metrics_dict = {scores : val_metrics[scores] for scores in metrics_table_left}
            metrics_table = [[metric_name,
                              metrics_dict[metric_name][3],
                            #   metrics_dict[metric_name][0],
                            #   metrics_dict[metric_name][1],
                            #   metrics_dict[metric_name][2]
                            ]
                             for metric_name in metrics_table_left
                            ]
            table_s = tabulate(metrics_table, headers=metrics_table_header, tablefmt='grid')
            loss_s = F"mean_loss : {val_mean_loss:.3f}  ðŸŽðŸŽðŸŽ\n"

            # è®°å½•æ¯ä¸ªepochå¯¹åº”çš„train_lossã€lrä»¥åŠéªŒè¯é›†å„æŒ‡æ ‡
            write_info = epoch_s + model_s + lr_s + wd_s + loss_fn_s + table_s + '\n' + loss_s + cost_s + time_s + '\n'

            # æ‰“å°ç»“æžœ
            print(write_info)

            # ä¿å­˜ç»“æžœ
            results_file = f"{args.model}_{log_name}.txt"
            file_path = os.path.join(save_scores_path, results_file)
            with open(file_path, "a") as f:
                f.write(write_info)
        # lossæ¸…é›¶

        if args.save_weights:
            # ä¿å­˜bestæ¨¡åž‹
            save_weights_path = f"results/save_weights/{args.model}"  # ä¿å­˜æƒé‡è·¯å¾„
            
            if not os.path.exists(save_weights_path):
                os.makedirs(save_weights_path)

            if current_mae >= val_mean_loss and current_f1 <= val_metrics["F1_scores"][3]:
                torch.save(save_file, f"{save_weights_path}/model_best.pth")

            # only save latest 10 epoch weights
            if os.path.exists(f"{save_weights_path}/model_ep:{epoch-10}.pth"):
                os.remove(f"{save_weights_path}/model_ep:{epoch-10}.pth")
                
            if not os.path.exists(save_weights_path):
                os.makedirs(save_weights_path)
            torch.save(save_file, f"{save_weights_path}/model_ep:{epoch}.pth")

    total_time = time.time() - initial_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print("====training over. total time: {}".format(total_time_str))
        



if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="train model on SEM stone dataset")

    parser.add_argument('--data_path', type=str, default="/mnt/c/VScode/WS-Hub/WS-U2net/U-2-Net/SEM_DATA/CSV/rock_sem_320.csv", help="path to csv dataset")
    
    parser.add_argument('--model', type=str, default="unet", help="'u2net_full' or 'u2net_lite' or 'unet'")
    parser.add_argument('--loss_fn', type=str, default='FocalLoss', help="'CrossEntropyLoss', 'FocalLoss', 'DiceLoss'.")
    parser.add_argument('--optimizer', type=str, default='AdamW', help="'AdamW', 'SGD' or 'RMSprop'.")
    parser.add_argument('--save_scores_path', type=str, default='results/save_scores', help="root path to save scores on training and valing")
    parser.add_argument('--device', type=str, default='cuda:0')
    parser.add_argument('--amp', type=bool, default=True, help='use mixed precision training or not')
    parser.add_argument('--tb', type=bool, default=False, help='use tensorboard or not')   
    parser.add_argument('--split_flag', type=bool, default=True, help='split data or not')
    
    parser.add_argument('--save_weights', type=bool, default=True, help='save weights or not')
    
    parser.add_argument('--train_ratio', type=float, default=0.8)
    parser.add_argument('--val_ratio', type=float, default=0.1)
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--start_epoch', type=int, default=0, help='start epoch')
    parser.add_argument('--end_epoch', type=int, default=150, help='ending epoch')
    parser.add_argument('--lr', type=float, default=0.001, help='learning rate')
    parser.add_argument('--wd', type=float, default=1e-4, help='weight decay')
    parser.add_argument('--eval_interval', type=int, default=10, help='interval for evaluation')
    parser.add_argument('--small_data', type=int, default=None, help='number of small data')
  

    args = parser.parse_args()
    main(args)