import torch
import numpy as np
import torch.nn as nn

# kaimingåˆå§‹åŒ–
import torch.nn as nn
import torch.nn.init as init
import warnings
from collections import defaultdict

def kaiming_initial(model, a=0, mode='fan_in', nonlinearity='relu', 
                conv_type='2d', bias_strategy='zero', verbose=True):
    """
    ğŸ¯ åŠŸèƒ½ç‰¹ç‚¹ï¼š
    - æ”¯æŒå¤šç»´åº¦å·ç§¯å±‚ (1D/2D/3D)
    - è‡ªåŠ¨é€‚é…BNå±‚/LayerNormç­‰å½’ä¸€åŒ–å±‚
    - æ™ºèƒ½å¤„ç†è‡ªå®šä¹‰å±‚ (Mamba/Attention/DeformableConvç­‰)
    - é˜²å¾¡å¼å‚æ•°æ£€æµ‹ (æƒé‡/åç½®å­˜åœ¨æ€§æ ¡éªŒ)
    - åˆå§‹åŒ–è¿‡ç¨‹å¯è§†åŒ–è·Ÿè¸ª
    """
    
    # åˆå§‹åŒ–ç»Ÿè®¡å™¨
    init_stats = defaultdict(int)
    
    # éå†æ‰€æœ‰ç½‘ç»œå±‚
    for name, module in model.named_modules():
        # è·³è¿‡ç©ºå±‚å’Œå®¹å™¨å±‚
        if isinstance(module, (nn.ModuleList, nn.Sequential)): 
            continue
            
        # æ ¸å¿ƒåˆå§‹åŒ–é€»è¾‘ ===============================================
        # Case 1: å·ç§¯å±‚ç³»åˆ—
        if isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
            nn.init.kaiming_normal_(module.weight, a=a, mode=mode, 
                                  nonlinearity=nonlinearity)
            init_stats[f'Conv{conv_type}'] += 1
            
            # åç½®å¤„ç†ç­–ç•¥
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
                init_stats['ConvBias'] +=1
        # Case 2: è½¬ç½®å·ç§¯
        elif isinstance(module, (nn.ConvTranspose1d, nn.ConvTranspose2d, 
                               nn.ConvTranspose3d)):
            nn.init.kaiming_normal_(module.weight, a=a, mode='fan_out',
                                  nonlinearity=nonlinearity)
            init_stats['ConvTranspose'] +=1
            if module.bias is not None:
                nn.init.zeros_(module.bias)
                init_stats['ConvTransposeBias'] +=1
        # Case 3: å…¨è¿æ¥å±‚
        elif isinstance(module, nn.Linear):
            nn.init.kaiming_normal_(module.weight, mode='fan_out',
                                  nonlinearity=nonlinearity)
            init_stats['Linear'] +=1
            if module.bias is not None:
                nn.init.zeros_(module.bias)
                init_stats['LinearBias'] +=1
        # Case 4: æ‰¹å½’ä¸€åŒ–å±‚
        elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, 
                               nn.BatchNorm3d)):
            if module.weight is not None:
                nn.init.ones_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
            init_stats['BatchNorm'] +=1
        # Case 5: å±‚å½’ä¸€åŒ–
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
            init_stats['LayerNorm'] +=1
        # Case 6: è‡ªå®šä¹‰å±‚æ™ºèƒ½å¤„ç†ï¼ˆMamba/Attentionç­‰ï¼‰
        elif 'Mamba' in str(type(module)):
            # Mambaå±‚ç‰¹æ®Šåˆå§‹åŒ–ï¼ˆç¤ºä¾‹ï¼‰
            for param in module.parameters():
                if param.dim() >= 2:
                    nn.init.xavier_uniform_(param)
                else:
                    nn.init.normal_(param, mean=0, std=0.01)
            init_stats['MambaLayer'] +=1
            
        # elif 'Attention' in str(type(module)):
        #     # æ³¨æ„åŠ›æœºåˆ¶åˆå§‹åŒ–
        #     nn.init.xavier_uniform_(module.q_proj.weight)
        #     nn.init.xavier_uniform_(module.k_proj.weight)
        #     nn.init.xavier_uniform_(module.v_proj.weight)
        #     init_stats['Attention'] +=1
        # Case 7: é˜²å¾¡å¼å…œåº•ç­–ç•¥
        else:
            # å‚æ•°å­˜åœ¨æ€§æ ¡éªŒ
            has_weight = hasattr(module, 'weight') and module.weight is not None
            has_bias = hasattr(module, 'bias') and module.bias is not None
            
            # æƒé‡åˆå§‹åŒ–
            if has_weight:
                if module.weight.dim() >= 2:
                    nn.init.kaiming_normal_(module.weight, a=a, mode=mode,
                                          nonlinearity=nonlinearity)
                else:
                    nn.init.normal_(module.weight, mean=0, std=0.01)
                init_stats['FallbackWeight'] +=1
                
            # åç½®åˆå§‹åŒ–
            if has_bias:
                if bias_strategy == 'zero':
                    nn.init.zeros_(module.bias)
                else:
                    nn.init.normal_(module.bias, mean=0, std=0.01)
                init_stats['FallbackBias'] +=1
            # è­¦å‘Šæœªè¯†åˆ«å±‚
            if verbose and (has_weight or has_bias):
                warnings.warn(f"âš ï¸ æœªæ³¨å†Œå±‚ç±»å‹ [{type(module).__name__}] "+
                            f"è·¯å¾„: {name} å·²æ‰§è¡Œé˜²å¾¡å¼åˆå§‹åŒ–", 
                            UserWarning, stacklevel=2)
    
    # æ‰“å°åˆå§‹åŒ–æŠ¥å‘Š               
    if verbose:
        print("\nğŸ”¥ åˆå§‹åŒ–ç»Ÿè®¡æŠ¥å‘Šï¼š")
        for k, v in init_stats.items():
            print(f"â–¸ {k.ljust(20)} : {v}")
        print(f"âœ… æ€»åˆå§‹åŒ–å‚æ•°æ•°é‡: {sum(init_stats.values())}")
    
    return model

# æ­£äº¤åˆå§‹åŒ–
def init_weights_2d(m):
    """å¢å¼ºé²æ£’æ€§çš„2Dæƒé‡åˆå§‹åŒ–å‡½æ•°"""
    if isinstance(m, (nn.Conv2d)):
        nn.init.kaiming_normal_(
            m.weight, 
            mode='fan_in',
            nonlinearity='relu',
            a=0
        )
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, (nn.ConvTranspose2d)):
        nn.init.kaiming_normal_(
            m.weight,
            mode='fan_out',
            nonlinearity='relu',
            a=0
        )
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)
        
    # è§„èŒƒåŒ–å±‚å¤„ç†ç»Ÿä¸€åŒ–
    elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d, nn.GroupNorm)):
        if m.affine:  # ä»…å½“å…·æœ‰å¯å­¦ä¹ å‚æ•°æ—¶è¿›è¡Œåˆå§‹åŒ–
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
            # å¯¹InstanceNormå¢åŠ é¢å¤–åˆå§‹åŒ–
            if isinstance(m, nn.InstanceNorm2d):
                # åœ¨å¸¸æ•°åˆå§‹åŒ–åŸºç¡€ä¸Šå¢åŠ å°æ‰°åŠ¨
                nn.init.normal_(m.weight, mean=1.0, std=0.01)

# xavieråˆå§‹åŒ–
def xavier_initial(model, init_gain=0.02):
    for m in model.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.xavier_uniform_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, init_gain)
            nn.init.constant_(m.bias, 0)