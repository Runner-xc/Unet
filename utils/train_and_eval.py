from tqdm import tqdm
from torch.amp import GradScaler, autocast
import torch
import numpy as np
from metrics import *
import torch.nn.functional as F
"""
è®­ç»ƒå’ŒéªŒè¯
"""
def total_loss(inputs, target, loss_fn):
    """
    inputs: é¢„æµ‹å€¼
    target: çœŸå®å€¼
    loss_fn: æŸå¤±å‡½æ•°
    """
    # è·å–æ€»çš„æŸå¤±
    loss_list = [loss_fn(inputs[i], target) for i in range(len(inputs))]
    mean_loss_list = []
    OM_loss_list = []
    OP_loss_list = []
    IOP_loss_list = []

    # éå†æ¯ä¸€å±‚æŸå¤±
    for loss in loss_list: 
        OM_loss, OP_loss, IOP_loss, mean_loss = loss   # ä½¿ç”¨ *rest æ¥æ•è·é¢å¤–çš„æŸå¤±
        mean_loss_list.append(mean_loss)
        OM_loss_list.append(OM_loss)
        OP_loss_list.append(OP_loss)
        IOP_loss_list.append(IOP_loss)
    
    # è®¡ç®—å¹³å‡æŸå¤±
    train_loss = sum(mean_loss_list) / len(mean_loss_list)
    OM_loss = sum(OM_loss_list) / len(OM_loss_list)
    OP_loss = sum(OP_loss_list) / len(OM_loss_list)
    IOP_loss = sum(IOP_loss_list) / len(OM_loss_list)

    return OM_loss, OP_loss, IOP_loss, train_loss

def train_one_epoch(model, optimizer, epoch, train_dataloader, device, loss_fn, scaler):
    """"
    
    
    """
    
    model.train()
    
    train_loss = 0.0
    OM_loss = 0.0
    OP_loss = 0.0
    IOP_loss = 0.0

    # ä½¿ç”¨ tqdm åŒ…è£… train_dataloader
    train_dataloader = tqdm(train_dataloader, desc=f" Training on Epoch :{epoch}ğŸ˜€", leave=False)
    
    for data in train_dataloader: 
        # è·å–è®­ç»ƒæ•°æ®é›†çš„ä¸€ä¸ªbatch
        images, masks = data[0].to(device), data[1].to(device)
        # æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()
        # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        with autocast(device_type="cuda"):
            # è®­ç»ƒ + è®¡ç®—loss
            pred_masks = model(images)
            train_OM_loss, train_OP_loss, train_IOP_loss, train_mean_loss = total_loss(pred_masks, masks, loss_fn)

        # åå‘ä¼ æ’­
        scaler.scale(train_mean_loss).backward()
        
        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åŒ…å«infæˆ–nan
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # æ›´æ–°å‚æ•°
        scaler.step(optimizer)
        
        # æ›´æ–°æ¢¯åº¦ç¼©æ”¾å™¨
        scaler.update()
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
        train_dataloader.update()

        # ç´¯åŠ æŸå¤±
        train_loss += train_mean_loss.item()
        OM_loss += train_OM_loss.item()
        OP_loss += train_OP_loss.item()
        IOP_loss += train_IOP_loss.item()
    
    return train_loss, OM_loss, OP_loss, IOP_loss

def evaluate(model, device, data_loader, loss_fn, Metric):
    """
    
    """
    model.eval()
    Metric_list = np.zeros((4, 4))
    val_mean_loss = 0.0
    val_OM_loss = 0.0
    val_OP_loss = 0.0
    val_IOP_loss = 0.0


    with torch.no_grad():
        val_dataloader = tqdm(data_loader, desc=f"  Validating  ğŸ˜€", leave=False)
        for data in val_dataloader:
            images, masks =data[0].to(device), data[1].to(device)
            with autocast(device_type="cuda"):
                pred_masks = model(images)
                OM_loss, OP_loss, IOP_loss, mean_loss = loss_fn(pred_masks, masks)
                metrics = Metric.update(pred_masks, masks)
                Metric_list += metrics

            # ç´¯åŠ æŸå¤±
            val_mean_loss += mean_loss.item()
            val_OM_loss += OM_loss.item()
            val_OP_loss += OP_loss.item()
            val_IOP_loss += IOP_loss.item()
    
    Metric_list /= len(val_dataloader)

    return val_OM_loss, val_OP_loss, val_IOP_loss, val_mean_loss, Metric_list